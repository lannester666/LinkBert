{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import json\n",
    "import re\n",
    "import regex\n",
    "import jsonlines\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data reading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_raw_task1_data(task1_data_dir, task1_xlsx_path):\n",
    "    triad_df = pd.read_excel(task1_xlsx_path)\n",
    "\n",
    "    task1_raw_data = {}\n",
    "    for file_name in sorted(os.listdir(task1_data_dir)):\n",
    "        if file_name.endswith('.xlsx'):\n",
    "            continue\n",
    "\n",
    "        # json文件读取\n",
    "        file_path = os.path.join(task1_data_dir, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # xlsx文件读取\n",
    "        source_id = int(data[\"sourceid\"])\n",
    "        matching_rows = triad_df[triad_df['PMID'] == source_id]\n",
    "        tri = []\n",
    "        for _, row in matching_rows.iterrows():\n",
    "            if (pd.isna(row['GENE']) or pd.isna(row['FUNCTION']) or pd.isna(row['DISEASE'])): continue  # 有些样本的数据为空\n",
    "            gene_text = row['GENE']\n",
    "            func_text = row['FUNCTION']\n",
    "            disease_text = row['DISEASE']\n",
    "            tri.append((gene_text, func_text, disease_text))\n",
    "        data['triplets'] = tri\n",
    "\n",
    "        task1_raw_data[data[\"sourceid\"]] = data\n",
    "\n",
    "    return task1_raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_processed_task1_data(file_path):\n",
    "    # Read the JSON file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # 提取元组\n",
    "    texts = []\n",
    "    triplets = []\n",
    "    for element in data:\n",
    "        texts.append(element['input'])\n",
    "        triplet_string = element['output']\n",
    "        tris = regex.findall(r'\\(([^,]+),\\s*([^,]+),\\s*([^,]+)\\)', triplet_string)\n",
    "        triplets.append(tris)\n",
    "    ids = list(range(len(texts)))\n",
    "    \n",
    "    return ids, texts, triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_processed_task2_data(file_path):\n",
    "    # Read the JSON file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # 提取元组\n",
    "    texts = []\n",
    "    triplets = []\n",
    "    for element in data:\n",
    "        texts.append(element['input'])\n",
    "        triplet_string = element['response']\n",
    "        tris = regex.findall(r'\\(([^,]+),\\s*([^,]+)\\)', triplet_string)\n",
    "        triplets.append(tris)\n",
    "    ids = list(range(len(texts)))\n",
    "    \n",
    "    return ids, texts, triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_processed_task3_data(file_path):\n",
    "    # Read the JSON file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # 提取元组\n",
    "    texts = []\n",
    "    triplets = []\n",
    "    for element in data:\n",
    "        texts.append(element['input'])\n",
    "        triplet_string = element['response']\n",
    "        tris = regex.findall(r'\\(([^,]+),\\s*([^,]+),\\s*([^,]+)\\)', triplet_string)\n",
    "        triplets.append(tris)\n",
    "    ids = list(range(len(texts)))\n",
    "    \n",
    "    return ids, texts, triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_submission_data(file_path):\n",
    "    # Read the JSON file\n",
    "    data = []\n",
    "    with jsonlines.open(file_path, \"r\") as reader:\n",
    "        for read_line in reader:\n",
    "            data.append(read_line)\n",
    "\n",
    "    # 提取元组\n",
    "    texts = []\n",
    "    triplets = []\n",
    "    for item in data:\n",
    "        if item['task'] == 1:\n",
    "            text = item['text']\n",
    "            triplet_string = item['ideal'][\"GENE, FUNCTION, DISEASE\"]\n",
    "            tris = regex.findall(r'\\(([^,]+),\\s*([^,]+),\\s*([^,]+)\\)', triplet_string)\n",
    "        elif item['task'] == 2:\n",
    "            text = item['abstract']\n",
    "            triplet_string = item['ideal'][\"chemical, disease\"]\n",
    "            tris = regex.findall(r'\\(([^,]+),\\s*([^,]+)\\)', triplet_string)\n",
    "        elif item['task'] == 3:\n",
    "            text = item['text']\n",
    "            triplet_string = item['ideal'][\"DDI-triples\"]\n",
    "            tris = regex.findall(r'\\(([^,]+),\\s*([^,]+),\\s*([^,]+)\\)', triplet_string)\n",
    "        texts.append(text)\n",
    "        \n",
    "        triplets.append(tris)\n",
    "    ids = list(range(len(texts)))\n",
    "    \n",
    "    return ids, texts, triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_by_merge(factory_input_file, factory_output_file):\n",
    "    # Read the JSON file\n",
    "    output_data = []\n",
    "    with jsonlines.open(factory_output_file, \"r\") as reader:\n",
    "        for read_line in reader:\n",
    "            output_data.append(read_line)\n",
    "    \n",
    "    with open(factory_input_file, \"r\") as file:\n",
    "        input_data = json.load(file)\n",
    "\n",
    "    # 提取元组\n",
    "    texts = []\n",
    "    triplets = []\n",
    "    for o, i in zip(output_data, input_data):\n",
    "        texts.append(i['input'])\n",
    "        triplet_string = o['predict']\n",
    "        tris = regex.findall(r'\\(([^,]+),\\s*([^,]+),\\s*([^,]+)\\)', triplet_string)\n",
    "        triplets.append(tris)\n",
    "    ids = list(range(len(texts)))\n",
    "\n",
    "    return ids, texts, triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo(ids, texts, triples, aux_triplets=None):\n",
    "    # 生成颜色列表\n",
    "    cmap = plt.get_cmap('rainbow')\n",
    "    colors = [cmap(i) for i in np.linspace(0, 1, 50)]\n",
    "    colors = ['#{:02x}{:02x}{:02x}'.format(int(c[0]*255), \n",
    "                                            int(c[1]*255), \n",
    "                                            int(c[2]*255)) for c in colors]\n",
    "\n",
    "\n",
    "    # 定义更新文本和元组的函数\n",
    "    def update_display(change):\n",
    "        index = change[\"new\"]\n",
    "        text = texts[index]\n",
    "        tris = triples[index]\n",
    "        if aux_triplets is not None:\n",
    "            aux_tris = aux_triplets[index]\n",
    "\n",
    "        # 高亮显示文本中的实体\n",
    "        highlighted_sents = sent_tokenize(text)\n",
    "        entities = [en for tri in tris for en in tri]\n",
    "        if aux_triplets:\n",
    "            entities += [en for tri in aux_tris for en in tri]\n",
    "        entities = list(set(entities))\n",
    "        entities = sorted(entities, key=len, reverse=True)  # 先着色长实体，再着色短实体，因为存在重叠\n",
    "        for i in range(len(highlighted_sents)):\n",
    "            for entity in entities:\n",
    "                highlighted_sents[i] = highlighted_sents[i].replace(entity, \n",
    "                                                            f\"<span style='background-color:\\\n",
    "                                                            {colors[hash(entity) % len(colors)]}'>{entity}</span>\")\n",
    "        \n",
    "        # 高亮显示每个元组\n",
    "        highlighted_triplets = []\n",
    "        for tri in tris:\n",
    "            line = '(' + ', '.join([\n",
    "                f\"<span style='background-color: {colors[hash(t) % len(colors)]}'>{t}</span>\" for t in tri\n",
    "                ]) + ')'\n",
    "            highlighted_triplets.append(line)\n",
    "\n",
    "        # 更新显示\n",
    "        text_widget.value = \"<br>\".join(highlighted_sents)\n",
    "        triplets_widget.value = \"<br>\".join(highlighted_triplets)\n",
    "        source_text_widget.value = text\n",
    "\n",
    "        # 辅助元组（optional）\n",
    "        if aux_triplets:\n",
    "            highlighted_aux_triplets = []\n",
    "            for tri in aux_tris:\n",
    "                line = '(' + ', '.join([\n",
    "                    f\"<span style='background-color: {colors[hash(t) % len(colors)]}'>{t}</span>\" for t in tri\n",
    "                    ]) + ')'\n",
    "                highlighted_aux_triplets.append(line)\n",
    "            aux_triplets_widget.value = \"<br>\".join(highlighted_aux_triplets)\n",
    "\n",
    "\n",
    "    dropdown = widgets.Dropdown(options=ids, description='Sample:')\n",
    "    dropdown.observe(update_display, 'value')\n",
    "\n",
    "\n",
    "    def on_toggle_button_click(b):\n",
    "        current_index = dropdown.options.index(dropdown.value)\n",
    "        next_index = (current_index + 1) % len(dropdown.options)\n",
    "        dropdown.value = dropdown.options[next_index]\n",
    "\n",
    "\n",
    "    def on_toggle_button_click_r(b):\n",
    "        current_index = dropdown.options.index(dropdown.value)\n",
    "        next_index = (current_index - 1) % len(dropdown.options)\n",
    "        dropdown.value = dropdown.options[next_index]\n",
    "\n",
    "\n",
    "    toggle_button = widgets.Button(description=\"Next\")\n",
    "    toggle_button.on_click(on_toggle_button_click)\n",
    "    toggle_button_r = widgets.Button(description=\"Last\")\n",
    "    toggle_button_r.on_click(on_toggle_button_click_r)\n",
    "\n",
    "    hbox = widgets.HBox([dropdown, toggle_button_r, toggle_button])\n",
    "    text_widget = widgets.HTML(layout=widgets.Layout(width='100%', height='300px'), description=\"Text:\")\n",
    "    triplets_widget = widgets.HTML(layout=widgets.Layout(width='50%', height='200px'), description=\"Pairs:\")\n",
    "    aux_triplets_widget = widgets.HTML(layout=widgets.Layout(width='50%', height='200px'), description=\"Aux pairs:\")\n",
    "    triplets_hbox = widgets.HBox([triplets_widget, aux_triplets_widget])\n",
    "    source_text_widget = widgets.HTML(layout=widgets.Layout(width='100%', height='300px'), description=\"Source text:\")\n",
    "\n",
    "    # refresh the display\n",
    "    update_display({\"new\": 0})\n",
    "    # Display the widgets\n",
    "    display(hbox, text_widget, triplets_hbox, source_text_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_for_task1_raw_data(task1_raw_data):\n",
    "    ids = list(task1_raw_data.keys())\n",
    "\n",
    "    color_map = {\n",
    "        'Gene': '#BA55D3',\n",
    "        'Var': '#FFC0CB',\n",
    "        'Disease': '#6495ED',\n",
    "        'Reg': '#778899',\n",
    "        'NegReg': '#00FA9A',\n",
    "        'PosReg': '#FF0000',\n",
    "        'MPA': '#FFE4B5',\n",
    "        'CPA': '#FFE4B5',\n",
    "        'Interaction': '#FFE4B5',\n",
    "        'Protein': '#E6E6FA',\n",
    "        'Enzyme': '#F4A460',\n",
    "        'Pathway': '#FFE4B5',\n",
    "        'REG': '#778899',\n",
    "        'GOF': '#FF0000',\n",
    "        'LOF': '#00FA9A',\n",
    "        'COM': '#FFFF00'\n",
    "    }\n",
    "\n",
    "    # 定义更新文本和元组的函数\n",
    "    def update_display(change):\n",
    "\n",
    "        id = change[\"new\"]\n",
    "        data = task1_raw_data[id]\n",
    "        text = data['text']\n",
    "        denotations = data['denotations']\n",
    "        relations = data['relations']\n",
    "        tris = data['triplets']\n",
    "\n",
    "        # 生成关系图\n",
    "        G = nx.DiGraph()\n",
    "        for deno in denotations:\n",
    "            G.add_node(deno['id'], attrs=deno)\n",
    "        for rel in relations:\n",
    "            if rel['pred'] == 'CauseOf':\n",
    "                G.add_edge(rel['subj'], rel['obj'])\n",
    "            elif rel['pred'] == 'ThemeOf':\n",
    "                G.add_edge(rel['obj'], rel['subj'])\n",
    "            else:\n",
    "                raise ValueError()\n",
    "        node_labels = {\n",
    "            node: f\"[{G.nodes[node]['attrs']['obj']}] \"\n",
    "            + ' '.join(text[G.nodes[node]['attrs']['span']['begin']: G.nodes[node]['attrs']['span']['end']].split(' '))\n",
    "            for node in G.nodes()\n",
    "        }\n",
    "        node_colors = [color_map[G.nodes[node]['attrs']['obj']] for node in G.nodes()]\n",
    "\n",
    "        # 根据关系图建立更细粒度的标签\n",
    "        weakly_connected_components = list(nx.weakly_connected_components(G))\n",
    "        fine_grained_relations = []\n",
    "        for cc_ids in weakly_connected_components:\n",
    "            # cc_ids is a set of node id, we need node objects\n",
    "            cc = [G.nodes[node_id] for node_id in cc_ids]\n",
    "            cur_gene_node = []\n",
    "            cur_disease_node = []\n",
    "            cur_reg_node = []\n",
    "            cur_lof_node = []\n",
    "            cur_gof_node = []\n",
    "            for node in cc:\n",
    "                if node['attrs']['obj'] == 'Gene':\n",
    "                    cur_gene_node.append(node)\n",
    "                elif node['attrs']['obj'] == 'Disease':\n",
    "                    cur_disease_node.append(node)\n",
    "                elif node['attrs']['obj'] == 'Reg':\n",
    "                    cur_reg_node.append(node)\n",
    "                elif node['attrs']['obj'] == 'NegReg':\n",
    "                    cur_lof_node.append(node)\n",
    "                elif node['attrs']['obj'] == 'PosReg':\n",
    "                    cur_gof_node.append(node)\n",
    "            if len(cur_gene_node) == 0 or len(cur_disease_node) == 0:\n",
    "                continue\n",
    "            if len(cur_reg_node) == 0:\n",
    "                assert len(cur_lof_node) + len(cur_gof_node) == 1\n",
    "            label_rel_node = None\n",
    "            if cur_reg_node:\n",
    "                label_rel_node = cur_reg_node[0]\n",
    "            if cur_lof_node:\n",
    "                label_rel_node = cur_lof_node[0]\n",
    "            if cur_gof_node:\n",
    "                label_rel_node = cur_gof_node[0]\n",
    "            assert label_rel_node is not None\n",
    "            fine_grained_relations.extend([(g, label_rel_node, d) for g in cur_gene_node for d in cur_disease_node])\n",
    "        # 按照(gene_span_begin, disease_span_begin)升序排列\n",
    "        sorted(fine_grained_relations, key = lambda tri: (tri[0]['attrs']['span']['begin'], tri[2]['attrs']['span']['end']))\n",
    "\n",
    "        fine_grained_triplets = []\n",
    "        for tri in fine_grained_relations:\n",
    "            gene_text = text[tri[0]['attrs']['span']['begin']: tri[0]['attrs']['span']['end']] + ':GENE'\n",
    "            rel_text = text[tri[1]['attrs']['span']['begin']: tri[1]['attrs']['span']['end']]\n",
    "            if tri[1]['attrs']['obj'] == 'Reg': rel_text += ':REG'\n",
    "            elif tri[1]['attrs']['obj'] == 'NegReg': rel_text += ':LOF'\n",
    "            elif tri[1]['attrs']['obj'] == 'PosReg': rel_text += ':GOF'\n",
    "            disease_text = text[tri[2]['attrs']['span']['begin']: tri[2]['attrs']['span']['end']] + ':DISEASE'\n",
    "            fine_grained_triplets.append((gene_text, rel_text, disease_text))\n",
    "\n",
    "        with output_widget:\n",
    "            output_widget.clear_output(wait=True)\n",
    "            plt.clf()\n",
    "            current_figure = plt.gcf()\n",
    "            current_figure.set_size_inches(10, 4)\n",
    "            # pos = nx.spring_layout(G, seed=42)\n",
    "            pos = nx.shell_layout(G)\n",
    "            nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=400, alpha=0.6, linewidths=2)\n",
    "            nx.draw_networkx_edges(G, pos, arrows=True, alpha=1, width=1.8)\n",
    "            nx.draw_networkx_labels(G, pos, labels=node_labels, font_size=8, font_weight='bold', font_color='black')\n",
    "            plt.title(\"Directed Graph Visualization\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # 生成高亮文本\n",
    "        highlighted_text = text\n",
    "        # 从后向前替换，避开替换后idx改变的问题\n",
    "        for deno in sorted(denotations, key=lambda deno: deno['span']['begin'], reverse=True):\n",
    "            start_index = deno['span']['begin']\n",
    "            end_index = deno['span']['end']\n",
    "            highlighted_text = highlighted_text[:start_index] \\\n",
    "            + (f\"<span style='background-color:{color_map[deno['obj']]}'>{highlighted_text[start_index: end_index]}</span>\") \\\n",
    "            + (highlighted_text[end_index:])\n",
    "\n",
    "        # 生成高亮三元组文本（来自excel文件）\n",
    "        highlighted_triplets = []\n",
    "        for tri in tris:\n",
    "            line = '(' \\\n",
    "            + f\"<span style='background-color: {color_map['Gene']}'>{tri[0]}</span>\" \\\n",
    "            + ', ' \\\n",
    "            + f\"<span style='background-color: {color_map[tri[1]]}'>{tri[1]}</span>\" \\\n",
    "            + ', ' \\\n",
    "            + f\"<span style='background-color: {color_map['Disease']}'>{tri[2]}</span>\" \\\n",
    "            + ')'\n",
    "            highlighted_triplets.append(line)\n",
    "\n",
    "        # 生成辅助高亮三元组文本\n",
    "        highlighted_aux_triplets = []\n",
    "        for tri in fine_grained_triplets:\n",
    "            line = '(' \\\n",
    "            + f\"<span style='background-color: {color_map['Gene']}'>{tri[0]}</span>\" \\\n",
    "            + ', ' \\\n",
    "            + f\"<span style='background-color: {color_map[tri[1].split(':')[-1]]}'>{tri[1]}</span>\" \\\n",
    "            + ', ' \\\n",
    "            + f\"<span style='background-color: {color_map['Disease']}'>{tri[2]}</span>\" \\\n",
    "            + ')'\n",
    "            highlighted_aux_triplets.append(line)\n",
    "\n",
    "        # 更新显示\n",
    "        text_widget.value = \"<br><br>\".join(sent_tokenize(highlighted_text))\n",
    "        triplets_widget.value = \"<br>\".join(highlighted_triplets)\n",
    "        aux_triplets_widget.value = \"<br>\".join(highlighted_aux_triplets)\n",
    "        source_text_widget.value = text\n",
    "\n",
    "    dropdown = widgets.Dropdown(options=ids, description='Sample:')\n",
    "    dropdown.observe(update_display, 'value')\n",
    "\n",
    "\n",
    "    def on_toggle_button_click(b):\n",
    "        current_index = dropdown.options.index(dropdown.value)\n",
    "        next_index = (current_index + 1) % len(dropdown.options)\n",
    "        dropdown.value = dropdown.options[next_index]\n",
    "\n",
    "\n",
    "    def on_toggle_button_click_r(b):\n",
    "        current_index = dropdown.options.index(dropdown.value)\n",
    "        next_index = (current_index - 1) % len(dropdown.options)\n",
    "        dropdown.value = dropdown.options[next_index]\n",
    "\n",
    "\n",
    "    toggle_button = widgets.Button(description=\"Next\")\n",
    "    toggle_button.on_click(on_toggle_button_click)\n",
    "    toggle_button_r = widgets.Button(description=\"Last\")\n",
    "    toggle_button_r.on_click(on_toggle_button_click_r)\n",
    "\n",
    "    hbox = widgets.HBox([dropdown, toggle_button_r, toggle_button])\n",
    "    text_widget = widgets.HTML(layout=widgets.Layout(width='100%', height='250px'), description=\"Text:\")\n",
    "    triplets_widget = widgets.HTML(layout=widgets.Layout(width='50%', height='100px'), description=\"Pairs:\")\n",
    "    aux_triplets_widget = widgets.HTML(layout=widgets.Layout(width='50%', height='100px'), description=\"Aux pairs:\")\n",
    "    triplets_hbox = widgets.HBox([triplets_widget, aux_triplets_widget])\n",
    "    source_text_widget = widgets.HTML(layout=widgets.Layout(width='100%', height='500px'), description=\"Source text:\")\n",
    "    output_widget = widgets.Output(layout={'display': 'flex', 'justify_content': 'center'}, description=\"Graph:\")\n",
    "\n",
    "    # refresh the display\n",
    "    update_display({\"new\": ids[0]})\n",
    "    # Display the widgets\n",
    "    display(hbox, text_widget, triplets_hbox, output_widget, source_text_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo for processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids, texts, triplets = read_by_merge(\n",
    "#     \"/home/zty/ykd_workspace/llm/processed_data/task1_data.json\",\n",
    "#     \"/home/zty/ykd_workspace/LLaMA-Factory/saves/Gemma-7B/lora/eval_2024-03-23-baseline-refined-task1-data-task1-eval/generated_predictions.jsonl\",\n",
    "#     )\n",
    "# _, _, aux_triplets = read_processed_task1_data(\"/home/zty/ykd_workspace/llm/processed_data/task1_data.json\")\n",
    "\n",
    "# ids, texts, triplets = read_processed_task1_data(\"/home/zty/ykd_workspace/llm/processed_data/task1_data.json\")\n",
    "# ids, texts, triplets = read_processed_task2_data(\"/home/zty/ykd_workspace/llm/processed_data/task2_data.json\")\n",
    "# ids, texts, triplets = read_processed_task3_data(\"/home/zty/ykd_workspace/llm/processed_data/task3_data.json\")\n",
    "\n",
    "ids, texts, triplets = read_submission_data(\"/home/zhangtaiyan/workspace/comp/my_finetune/LLaMA-Factory/save/Qwen1.5-14B/pt/sft/full/2024-03-30-09-34-04-wo_A-1e-6/submission.jsonl\")\n",
    "_, _, aux_triplets = read_submission_data(\"/home/zhangtaiyan/workspace/comp/my_finetune/LLaMA-Factory/save/BioMistral-7B/pt/sft/full/2024-04-05-00-02-39-wo_A-1e-6/checkpoint-320/submission.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/zhangtaiyan/nltk_data'\n    - '/home/zhangtaiyan/anaconda3/envs/llama_factory/nltk_data'\n    - '/home/zhangtaiyan/anaconda3/envs/llama_factory/share/nltk_data'\n    - '/home/zhangtaiyan/anaconda3/envs/llama_factory/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# demo(ids, texts, triplets, None)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m demo(ids, texts, triplets, aux_triplets)\n",
      "Cell \u001b[0;32mIn[29], line 84\u001b[0m, in \u001b[0;36mdemo\u001b[0;34m(ids, texts, triples, aux_triplets)\u001b[0m\n\u001b[1;32m     81\u001b[0m source_text_widget \u001b[39m=\u001b[39m widgets\u001b[39m.\u001b[39mHTML(layout\u001b[39m=\u001b[39mwidgets\u001b[39m.\u001b[39mLayout(width\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m100\u001b[39m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m, height\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m300px\u001b[39m\u001b[39m'\u001b[39m), description\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSource text:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[39m# refresh the display\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m update_display({\u001b[39m\"\u001b[39;49m\u001b[39mnew\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0\u001b[39;49m})\n\u001b[1;32m     85\u001b[0m \u001b[39m# Display the widgets\u001b[39;00m\n\u001b[1;32m     86\u001b[0m display(hbox, text_widget, triplets_hbox, source_text_widget)\n",
      "Cell \u001b[0;32mIn[29], line 19\u001b[0m, in \u001b[0;36mdemo.<locals>.update_display\u001b[0;34m(change)\u001b[0m\n\u001b[1;32m     16\u001b[0m     aux_tris \u001b[39m=\u001b[39m aux_triplets[index]\n\u001b[1;32m     18\u001b[0m \u001b[39m# 高亮显示文本中的实体\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m highlighted_sents \u001b[39m=\u001b[39m sent_tokenize(text)\n\u001b[1;32m     20\u001b[0m entities \u001b[39m=\u001b[39m [en \u001b[39mfor\u001b[39;00m tri \u001b[39min\u001b[39;00m tris \u001b[39mfor\u001b[39;00m en \u001b[39min\u001b[39;00m tri]\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m aux_triplets:\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/anaconda3/envs/llama_factory/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/home/zhangtaiyan/nltk_data'\n    - '/home/zhangtaiyan/anaconda3/envs/llama_factory/nltk_data'\n    - '/home/zhangtaiyan/anaconda3/envs/llama_factory/share/nltk_data'\n    - '/home/zhangtaiyan/anaconda3/envs/llama_factory/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# demo(ids, texts, triplets, None)\n",
    "demo(ids, texts, triplets, aux_triplets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo for task1 raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1_raw_data = read_raw_task1_data('/home/zty/ykd_workspace/llm_data/task1_data', '/home/zty/ykd_workspace/llm_data/task1_data/train_triad.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0764bedf1a24c66b29c26c5950d97b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Sample:', options=('15678000', '15773749', '15773758', '15857086', '15880…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6e1b6c49824fefbdf5f0f0ae6c84bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value=\"[Autosomal dominant limb-girdle muscular dystrophy associated with conduction defects (LGMD1B): a …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8e1c2bfd2d4eff95d5494ae2985e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=\"(<span style='background-color: #BA55D3'>LMNA</span>, <span style='background-color…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69916aca6b744f71b379464657afd78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(display='flex', justify_content='center'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9ded01f633411e94163a841b2cc9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='[Autosomal dominant limb-girdle muscular dystrophy associated with conduction defects (LGMD1B): a …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo_for_task1_raw_data(task1_raw_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "34ec563793099652d03baa6f19273223fdf38f74992ff3874e47e238a522e895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
